{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MsITBHLCFCnJ"
   },
   "source": [
    "# Wstęp\n",
    "Zadanie 10 jest pierwszą częścią zajęć laboratoryjnych poświęconych sieciom rekurencyjnym i predykcji z wykorzystaniem danych multimodalnych. Efektem prac będzie sieć rekurencyjna do predykcji kursu kryptowaluty Bitcoin (BTC) w oparciu o dane z giełdy oraz o wyniki analizy emocji komunikatów z mediów społecznościowych, do których również należy utworzyć dedykowany model sieci rekurencyjnej. Plan realizacji etapów wygląda następująco:\n",
    "\n",
    "1.   EmoTweet - model sieci rekurencyjnej do analizy emocji (10 pkt., laboratorium 10)\n",
    "2. Agregacja informacji emotywnej i przygotowanie MultiBTC - multimodalnego model sieci rekurencyjnej do predykcji kursu BTC (10 pkt., laboratorium 11)\n",
    "3. Ewaluacja modelu MultiBTC (10 pkt., laboratorium 12)\n",
    "\n",
    "Łącznie można otrzymać 30 punktów.\n",
    "\n",
    "# Cel ćwiczenia\n",
    "\n",
    "Celem pierwszego etapu prac jest zapoznanie się z podstawową siecią rekurencyjną LSTM. Ze względu na fakt, że model ten będzie wykorzystany do analizy emocji tekstu, w ramach teorii do zadania zostanie omówiony podstawowy mechanizm konwersji słów w tekście do postaci wektorów dystrybucyjnych (tzw. word embeddings) na podstawie rozwiązania o nazwie `fastText`. Modele będą budowane na ogólnodostępnym zbiorze `TweetEval`, zawierającym podzbiory ręcznie anotowanych tweetów przy pomocy etykiet odnoszących się do następujących zjawisk: 1) emocje (emotion), 2) emotikony (emoji), 3) ironia (irony), 4) mowa nienawiści (hate speech), 5) mowa ofensywna (offensive language), 6) wydźwięk (sentiment), 7) nastawienie (stance). \n",
    "\n",
    "# Warunki zaliczenia\n",
    "\n",
    "Do zaliczenia pierwszego etapu należy utworzyć następujące modele dla min. 2 wybranych zjawisk:\n",
    "\n",
    "1.   Model bazowy (regresja logistyczna).\n",
    "2.   Model rekurencyjny oparty o sieć LSTM.\n",
    "\n",
    "Wytrenowane modele będą wykorzystane w 2 etapie, dlatego proszę je zachować.\n",
    "\n",
    "# Wektory dystrybucyjne\n",
    "\n",
    "W przetwarzaniu języka naturalnego, o wektorach dystrybucyjnych (inaczej osadzeniach lub zanurzeniach, ang. word embeddings) mówi się w kontekście reprezentacji słów w tekście, zazwyczaj w postaci wektora liczb rzeczywistych, który koduje znaczenie słowa. Hipoteza dystrybucyjna, u podstawy której leży większość metod reprezentacji, mówi o tym, że słowa, które często współwystępują, mają podobne znaczenie. Wektory dystrybucyjne można uzyskać za pomocą zestawu technik modelowania języka, w których słowa lub frazy są mapowane do wektorów liczb rzeczywistych. Z reguły polega to na matematycznym zanurzeniu z przestrzeni o wielu wymiarach opisujących słowo (konteksty) do ciągłej przestrzeni wektorowej o znacznie mniejszym wymiarze.\n",
    "\n",
    "Metody generowania tego odwzorowania obejmują sieci neuronowe, redukcję wymiarowości na macierzy współwystępowania słów, modele probabilistyczne lub jawną reprezentację w kontekście, w którym pojawiają się słowa. Wektory dystrybucyjne, używane jako podstawowa reprezentacja wejściowa tekstu, okazały się istotnie poprawiać jakość w wielu zadaniach NLP, takich jak np. rozpoznawanie nazw własnych, określanie części mowy, rozpoznawanie dziedziny tekstu, czy też rozpoznawanie wydźwięku i emocji w tekście. \n",
    "\n",
    "# fastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) jest biblioteką do efektywnego uczenia modeli reprezentacji wektorowych słów oraz do budowania klasyfikatorów tekstu. Modele językowe można budować z wykorzystaniem dwóch popularnych technik: [Continuous Bag of Words](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html) oraz [Skip-Gram](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c). \n",
    "\n",
    "## Instalacja\n",
    "\n",
    "Pobranie repozytorium projektu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UkMDUx6bn6e",
    "outputId": "eecff8d4-9a14-4409-f9c3-c66197d9ce6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fastText' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWc5Ie0cvYo"
   },
   "source": [
    "Instalacja biblioteki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ku0F_kKMbteg",
    "outputId": "f91d142f-9836-40d7-ef9e-d1727e8ff93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘build’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!cd fastText && mkdir build && cd build && cmake ..  && make && make install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7ApV6Bzwc1_R"
   },
   "source": [
    "Instalacja API do Pythona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xElxRxJycnDA",
    "outputId": "ee6668e8-b6c0-40be-d8b4-04d189c058e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /app/fastText\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (1.24.3)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext==0.9.2) (59.6.0)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4211752 sha256=23a4ee5b83bbf5b7c8d3c91cf93b7e8fe3fb55460295183d490dc1de94a3b8fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tkgw_mak/wheels/34/9a/79/ffb29de213ba6929d4e91e57b406e6f9ba1b7eba23dfa5c47d\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd fastText && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 1310...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.10/dist-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "help(fasttext.FastText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7ceW8c2XdOf4"
   },
   "source": [
    "# Dane do etapu nr 1\n",
    "\n",
    "## Korpus \n",
    "Korpus (zbiór dokumentów) do realizacji etapu nr 1 pochodzi z repozytorium [TweetEval](https://github.com/cardiffnlp/tweeteval). Repozytorium zawiera 7 różnorodnych zbiorów danych, zawierających zanonimizowane wpisy z [Twittera](https://twitter.com), anotowane następującymi zjawiskami: 1) emocje (emotion), 2) emotikony (emoji), 3) ironia (irony), 4) mowa nienawiści (hate speech), 5) mowa ofensywna (offensive language), 6) wydźwięk (sentiment), 7) nastawienie (stance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ER8c8zNmgE40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'dane' folder is visible.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/dane\"\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"The 'dane' folder is visible.\")\n",
    "else:\n",
    "    print(\"The 'dane' folder is not visible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qS6VxC26gUNt",
    "outputId": "9bf16419-9508-4cd9-d46a-6884ae6f3bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,8 CPUs 13th Gen Intel(R) Core(TM) i5-13400F (B06F2),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 17390348 bytes (17 MiB)\n",
      "\n",
      "Extracting archive: dane/tweeteval.7z\n",
      "--\n",
      "Path = dane/tweeteval.7z\n",
      "Type = 7z\n",
      "Physical Size = 17390348\n",
      "Headers Size = 1810\n",
      "Method = LZMA2:24\n",
      "Solid = +\n",
      "Blocks = 1\n",
      "\n",
      "      0% 27 - tweeteval/datasets/stance/atheis                                           10% 40 - tweeteval/.git/hooks/pre-applypatch.samp                                                   10% 51 - tweeteval/.git/logs/refs/heads/mai                                             39% 55 - tweeteval/.git/packed-re                                   49% 64 - tweeteval/datasets/emoji/train_labels.tx                                                   59% 68 - tweeteval/datasets/emotion/mapping.t                                               59% 78 - tweeteval/datasets/hate/train_labels.t                                                 82% 85 - tweeteval/datasets/irony/train_labels.tx                                                   82% 95 - tweeteval/datasets/offensive/val_text.tx                                                   96% 101 - tweeteval/datasets/sentiment/val_labels.t                                                     96% 111 - tweeteval/datasets/stance/atheism/train_labels.tx                                                             96% 122 - tweeteval/datasets/stance/feminist/test_text.tx                                                           96% 132 - tweeteval/datasets/stance/hillary/val_text.tx                                                         96% 143 - tweeteval/predictions/stance/abortion.t                                                  Everything is Ok\n",
      "\n",
      "Folders: 33\n",
      "Files: 115\n",
      "Size:       30563155\n",
      "Compressed: 17390348\n"
     ]
    }
   ],
   "source": [
    "!7za x dane/tweeteval.7z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb6c5nuvgiaF"
   },
   "source": [
    "## Zawartość korpusu\n",
    "\n",
    "W katalogu głównym (tweeteval) znajdują się następujące elementy:\n",
    "*   `datasets` - katalog ze zbiorami danych\n",
    "   * `emotion` - tweety anotowane emocjami \n",
    "     * `mapping.txt` - identyfikatory etykiet oraz ich opis\n",
    "     * `train_text.txt` - wpisy z Twittera (część ucząca)\n",
    "     * `train_labels.txt` - etykiety wpisów z Twittera (część ucząca)\n",
    "     * `test_*.txt, valid_*.txt` - j.w. (część testowa i walidacyjna)\n",
    "   * `emoji` - tweety anotowane emotikonami\n",
    "   * `...` - katalogi zawierające tweety anotowane pozostałymi zjawiskami\n",
    "*   `predictions` - katalog z przykładowymi predykcjami\n",
    "   * `emotion.txt` - etykiety modelu predykcyjnego dla części testowej danych `emotion`\n",
    "   * `emoji.txt` - j.w. dla cz. testowej danych `emoji`\n",
    "   * `...` - j.w. dla pozostałych danych\n",
    "*   `evaluation_script.py` - skrypt do ewaluacji \n",
    "\n",
    "## Model języka\n",
    "\n",
    "Na potrzeby zadania został przygotowany model Skip-Gram reprezentacji wektorowej słów, zbudowany na wielkim korpusie tweetów dotyczących kursu BTC. Wersja binarna tego modelu dostępna jest w 2 wariantach:\n",
    "* wektory 100-elementowe (1.7GB, fasttext_tweetmodel_btc_sg_100_en.bin)\n",
    "* wektory 20-elementowe (350MB, fasttext_tweetmodel_btc_sg_20_en.bin)\n",
    "\n",
    "Na potrzeby prezentacji przykładowego rozwiązania zostanie wykorzystany mniejszy model. Do realizacji ostatecznego rozwiązania należy wykorzystać większy model. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "stk-sYt9o6sD"
   },
   "source": [
    "# Model bazowy rozpoznawania emocji\n",
    "\n",
    "Model bazowy, zbudowany z wykorzystaniem narzędzia fastText (oparty o regresję logistyczną), będzie punktem wyjścia do porównania się z modelami opartymi o sieci LSTM, których skonstruowanie i ewaluacja na wybranych zadaniach będzie celem etapu nr 1. \n",
    "\n",
    "Pobranie mniejszego modelu reprezentacji języka tweetów:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChAehfcHggvF"
   },
   "outputs": [],
   "source": [
    "# należy wgrać plik z katalogu \"dane\" o nazwie fasttext_tweetmodel_btc_sg_20_en.bin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NkacglPdr96Y"
   },
   "source": [
    "Wydobycie słownika wektorów z binarnego modelu języka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\t /usr/bin/python3.10\r\n",
      "/usr/bin/python3-config  /usr/bin/python3.10-config\r\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/bin/python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CHyqkncyrZru"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/python3 fastText/python/doc/examples/bin_to_vec.py dane/fasttext_tweetmodel_btc_sg_20_en.bin > dane/fasttext_tweetmodel_btc_sg_20_en.vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_E_5UDKYyzwp"
   },
   "source": [
    "Dodanie prefiksu `__label__` do etykiet zbioru `emotion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OjUQvyKIsKV8"
   },
   "outputs": [],
   "source": [
    "!sed 's/^/__label__/g' tweeteval/datasets/emotion/train_labels.txt > train_labels_emo.txt\n",
    "!sed 's/^/__label__/g' tweeteval/datasets/emotion/test_labels.txt > test_labels_emo.txt\n",
    "!sed 's/^/__label__/g' tweeteval/datasets/emotion/val_labels.txt > val_labels_emo.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IlHv_VJ5zIHw"
   },
   "source": [
    "Przygotowanie zbioru uczącego, testowego i walidacyjnego w formacie `fastText`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nNdjvsT8siZ1"
   },
   "outputs": [],
   "source": [
    "!paste -d \" \" tweeteval/datasets/emotion/train_text.txt train_labels_emo.txt > train_emo.txt\n",
    "!paste -d \" \" tweeteval/datasets/emotion/test_text.txt test_labels_emo.txt > test_emo.txt\n",
    "!paste -d \" \" tweeteval/datasets/emotion/val_text.txt val_labels_emo.txt > val_emo.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ro5A5HAEzGNJ"
   },
   "source": [
    "Trenowanie modelu z wykorzystaniem wejścia `train_emo.txt`, z określeniem wyjściowej nazwy modelu `emo_model`, dla wektorów słów o wymiarze `20`, z wykorzystaniem pretrenowanych wektorów z pliku `fasttext_tweetmodel_btc_sg_20_en.vec` i z uruchomieniem dostrajania hiperparametrów na zbiorze walidacyjnym `val_emo.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/fastText/build/fasttext\n",
      "/home/jovyan/fastText/build/lib.linux-x86_64-3.10/fasttext\n",
      "/home/jovyan/fastText/build/temp.linux-x86_64-3.10/python/fasttext_module/fasttext\n",
      "/home/jovyan/fastText/python/fasttext_module/fasttext\n",
      "/usr/local/lib/python3.10/dist-packages/fasttext\n",
      "/usr/local/bin/fasttext\n",
      "/app/fastText/build/fasttext\n",
      "/app/fastText/build/lib.linux-x86_64-3.10/fasttext\n",
      "/app/fastText/build/temp.linux-x86_64-3.10/python/fasttext_module/fasttext\n",
      "/app/fastText/python/fasttext_module/fasttext\n"
     ]
    }
   ],
   "source": [
    "!find / -name fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GChCYj1ptoEj",
    "outputId": "6948a803-c679-484b-e6e1-62f622c38bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   60 Best score:  0.692513 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  12887\n",
      "Number of labels: 4\n",
      "Progress: 100.0% words/sec/thread:  499013 lr:  0.000000 avg.loss:  0.572941 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "!/home/jovyan/fastText/build/fasttext supervised -input train_emo.txt -output emo_model -dim 20 -pretrainedVectors dane/fasttext_tweetmodel_btc_sg_20_en.vec -autotune-validation val_emo.txt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pkp4BDnvzxrN"
   },
   "source": [
    "Podstawowa ewaluacja modelu z wykorzystaniem `fastText`, wynikiem jest precyzja (P - precision) i kompletność (R - recall) w wariancie [weighted](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90GbSCCgvmCj",
    "outputId": "8d6a82fa-3a33-4a4f-f5cc-fad0c8b6c82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1421\r\n",
      "P@1\t0.699\r\n",
      "R@1\t0.699\r\n"
     ]
    }
   ],
   "source": [
    "!/home/jovyan/fastText/build/fasttext test emo_model.bin test_emo.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OoqaR5To0UWq"
   },
   "source": [
    "Rozszerzona ewaluacja modelu z wykorzystaniem `fastText`, wynikiem jest precyzja (P - precision), kompletność (R - recall) oraz F1-score dla każdej etykiety w wariancie [weighted](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3xFd-RDwv_w",
    "outputId": "1ff6b4ab-b956-49ca-cfc2-527e39933bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score : 0.771104  Precision : 0.704748  Recall : 0.851254   __label__0\r\n",
      "F1-Score : 0.682477  Precision : 0.687003  Recall : 0.678010   __label__3\r\n",
      "F1-Score : 0.670769  Precision : 0.746575  Recall : 0.608939   __label__1\r\n",
      "F1-Score : 0.407960  Precision : 0.525641  Recall : 0.333333   __label__2\r\n",
      "N\t1421\r\n",
      "P@1\t0.699\r\n",
      "R@1\t0.699\r\n"
     ]
    }
   ],
   "source": [
    "!/home/jovyan/fastText/build/fasttext test-label emo_model.bin test_emo.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__FOjZO0jrw"
   },
   "source": [
    "Przygotowanie danych do ewaluacji z wykorzystaniem skryptu dołączonego do zbioru TweetEval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dZyri0pVw43n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘predictions2’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1gMgdnILxScS"
   },
   "outputs": [],
   "source": [
    "!/home/jovyan/fastText/build/fasttext predict emo_model.bin tweeteval/datasets/emotion/test_text.txt | sed 's/__label__//g' > predictions2/emotion.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xRwh9aIx0s3I"
   },
   "source": [
    "Uruchomienie ewaluacji. Oprócz wyników P, R, F1 [weighted]((https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)) dla każdej etykiety, otrzymujemy również wyniki w wariancie [macro]((https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)). **Ostateczną miarą (TweetEval Score) jest miara F1-score w wariancie macro i tę miarę proszę traktować jako kluczową przy porównywaniu rozwiązań.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpAjMNr2xn-e",
    "outputId": "7a9d4908-2440-40ae-c24f-5a6ac8d4aaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'precision': 0.7047477744807121, 'recall': 0.8512544802867383, 'f1-score': 0.771103896103896, 'support': 558}\r\n",
      "1 {'precision': 0.7465753424657534, 'recall': 0.6089385474860335, 'f1-score': 0.6707692307692307, 'support': 358}\r\n",
      "2 {'precision': 0.5256410256410257, 'recall': 0.3333333333333333, 'f1-score': 0.40796019900497504, 'support': 123}\r\n",
      "3 {'precision': 0.6870026525198939, 'recall': 0.6780104712041884, 'f1-score': 0.6824769433465085, 'support': 382}\r\n",
      "accuracy 0.6988036593947924\r\n",
      "macro avg {'precision': 0.6659916987768463, 'recall': 0.6178842080775734, 'f1-score': 0.6330775673061526, 'support': 1421}\r\n",
      "weighted avg {'precision': 0.6950120268679963, 'recall': 0.6988036593947924, 'f1-score': 0.6905676674717359, 'support': 1421}\r\n",
      "------------------------------\r\n",
      "TweetEval Score (emotion): 0.6330775673061526\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/python3 tweeteval/evaluation_script.py --tweeteval_path tweeteval/datasets --predictions_path predictions2 --task emotion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FV9_Gnrz2T0m"
   },
   "source": [
    "# Budowa modeli EmoTweet\n",
    "\n",
    "W tej sekcji Państwa zadaniem będzie przygotowanie modeli sieci LSTM oraz modeli bazowych opartych o regresję logistyczną (fastText) dla wybranych 2 zjawisk ze zbioru TweetEval. Dla sieci LSTM kolejne jednostki sieci rekurencyjnej na wejściu dostają reprezentację wektorową kolejnych wyrazów w tekście. Wyjście z ostatniej jednostki podlega klasyfikacji. W celu usprawnienia zadania, przedstawiona zostanie metoda reprezentacji wektorowej tekstu z wykorzystaniem Pythonowego API do narzędzia fastText. Do ewaluacji modeli należy wykorzystać uprzednio zaprezentowany skrypt `tweeteval/evaluation_script.py`.\n",
    "\n",
    "## Wektoryzacja tekstu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pmdakZza43X8"
   },
   "outputs": [],
   "source": [
    "# inicjalizacja biblioteki\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DpJZYWyd6EK-"
   },
   "outputs": [],
   "source": [
    "# ładowanie modelu\n",
    "MODEL_PATH = 'dane/fasttext_tweetmodel_btc_sg_20_en.bin'\n",
    "model = fasttext.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "yRs60cO96zk5",
    "outputId": "11c62264-8f71-4f36-9222-038482fa4184"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>I get discouraged because I try for 5 fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>The @user are in contention and hosting @user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>@user @user @user @user @user as a fellow UP g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>You have a #problem? Yes! Can you do #somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>@user @user i will fight this guy! Don't insul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3257 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     “Worry is a down payment on a problem you may ...\n",
       "1     My roommate: it's okay that we can't spell bec...\n",
       "2     No but that's so cute. Atsu was probably shy a...\n",
       "3     Rooneys fucking untouchable isn't he? Been fuc...\n",
       "4     it's pretty depressing when u hit pan on ur fa...\n",
       "...                                                 ...\n",
       "3252  I get discouraged because I try for 5 fucking ...\n",
       "3253  The @user are in contention and hosting @user ...\n",
       "3254  @user @user @user @user @user as a fellow UP g...\n",
       "3255  You have a #problem? Yes! Can you do #somethin...\n",
       "3256  @user @user i will fight this guy! Don't insul...\n",
       "\n",
       "[3257 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wczytanie danych treningowych\n",
    "import pandas as pd\n",
    "TRAIN_PATH = 'tweeteval/datasets/emotion/train_text.txt'\n",
    "train_texts = pd.read_csv(TRAIN_PATH, sep='\\t', header=None)\n",
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fk-7nV9x8C-Y",
    "outputId": "c6bda960-2c8b-48cc-9016-602e843ea79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Worry -1 [-0.04189867  0.15429688  0.96717507  1.3809655   0.49123076 -0.5447607\n",
      " -0.11276884  0.20356484 -1.0640966  -1.6616327   0.03930127 -0.7224096\n",
      "  0.21334486 -0.5872285   0.2898182   0.81751084 -1.6077403   1.8038087\n",
      "  0.4850348   1.0643197 ]\n",
      "is 6 [ 0.24099417  0.13544752  0.7251924   0.32544732  0.27421224  0.31903243\n",
      "  0.7501186   0.22853182 -0.91543657  0.08587569  0.13866538 -0.38624704\n",
      " -0.30637258  0.13666666 -0.43992838 -0.12443608 -1.0383893  -0.06567164\n",
      "  0.17007533 -0.16708991]\n",
      "a 7 [-0.00810981 -0.03934941  0.81658655  0.56301105  0.43812367  0.29547286\n",
      "  0.4691784   0.07483605 -0.58705056  0.28240088 -0.6339584  -0.16187707\n",
      " -0.23376046 -0.1245347   0.03071329 -0.07603034 -0.9066614  -0.07007706\n",
      "  0.4522892  -0.15033531]\n",
      "down 174 [ 0.9175071  -1.0815151   0.07119758  0.34226617  0.9607946   0.5973182\n",
      "  0.91058624 -0.32068744 -0.72137564  1.2241784  -0.1882128  -0.23591968\n",
      " -0.02596712 -0.10194965 -0.09553405  0.36303622  0.22354192  0.4901933\n",
      "  0.5405883   0.5965071 ]\n",
      "payment 556 [ 0.6073219  -0.00891357  0.7414747   1.3249576   0.07864746  1.4985372\n",
      "  0.4708811   1.4519942   0.13019626 -0.07420245 -0.83968335 -0.05105841\n",
      " -0.28754452  0.36899182  0.7846754  -0.891886   -0.6211444   0.49265763\n",
      " -0.11467575 -0.0749438 ]\n",
      "on 14 [-0.09723835  0.3496086   0.6910995   0.28277752  0.8975253  -0.01230987\n",
      "  0.67500156  0.02879165 -0.786506    0.602648   -0.39817798  0.543332\n",
      " -0.46127442 -0.23781577  0.0184579  -0.28403515 -0.3935916   0.3259461\n",
      "  0.0507571  -0.03765008]\n",
      "a 7 [-0.00810981 -0.03934941  0.81658655  0.56301105  0.43812367  0.29547286\n",
      "  0.4691784   0.07483605 -0.58705056  0.28240088 -0.6339584  -0.16187707\n",
      " -0.23376046 -0.1245347   0.03071329 -0.07603034 -0.9066614  -0.07007706\n",
      "  0.4522892  -0.15033531]\n",
      "problem 1224 [ 0.9236216   0.13799877  0.9784595   1.5032955   0.31760898  0.13017276\n",
      "  0.5282482   0.4380176  -0.556051    0.11004172 -0.53169626 -0.7698464\n",
      " -0.21855882  0.10204052  0.089779    0.04558972 -0.4503975  -0.40340552\n",
      "  0.1640271   0.30253166]\n",
      "you 18 [ 0.36285082  0.23824042  0.92288643  0.33069703  0.61494684  0.62680995\n",
      "  0.8206055   0.49254185 -0.44717252 -0.47944754 -0.852041   -0.63990945\n",
      "  0.02505241 -0.43478322 -0.06202105  0.16197506 -0.36003593  0.2883114\n",
      "  0.73949414  0.5215194 ]\n",
      "may 309 [-0.20453136 -0.15716666  0.2648741   0.91359925  0.8807271   0.45750532\n",
      "  0.78538996  0.15402626 -0.9378929   0.49042216 -0.3321735  -0.6558436\n",
      "  0.08917101 -0.42916426 -0.4275598   0.3072008  -0.45899373 -0.02519113\n",
      "  0.34117666  0.12960152]\n",
      "never 299 [ 0.4569879  -0.43546084  0.8783896   0.8701302   0.47704792  0.46891853\n",
      "  0.77218413  0.28471166 -0.35907164 -0.17841178 -0.6685155  -1.0739613\n",
      "  0.17042854 -0.6416858  -0.0809866   0.52535385 -0.38173252  0.22777203\n",
      "  0.5776487   0.47688836]\n",
      "have'. -1 [-0.14791821  0.27410123  0.6040076   1.4533241   0.27587754  0.37697735\n",
      "  0.8350882   0.08774029 -0.9036173  -0.12468915 -0.8917518  -0.99509364\n",
      " -0.1603566  -0.50394845 -0.6500326   0.21711075 -0.20765467  0.57654476\n",
      "  0.976134   -0.09989043]\n",
      " Joyce -1 [ 0.3620899  -0.21890068  0.6421599   0.4646088  -0.23233397  0.01426617\n",
      "  0.75234526  0.06882509 -0.7289002   0.84054786 -0.8729406  -0.38942376\n",
      " -0.813916    0.52664775 -0.22176157  0.6487463  -0.97409683  1.1249645\n",
      " -0.6657481  -0.39433447]\n",
      "Meyer. 512882 [ 0.21895045  0.3844226   1.1516806   0.94822884  0.27445677  0.14578705\n",
      " -0.22505678 -0.32554576  0.16166015 -0.0054998  -1.2656554  -0.60765606\n",
      " -0.70257545  1.0171603  -0.3287179   0.5872529  -0.9836187   1.2412837\n",
      "  0.30786654 -0.36381623]\n",
      "#motivation 4480 [ 0.3188765   1.6882232  -0.5868895   0.6897793   0.95900065  0.80034953\n",
      " -1.3323131   1.2530718   0.0942677   0.2178247  -2.2693706  -0.7811642\n",
      "  0.01969428 -0.19465806 -1.6017233   1.0578989  -1.521254    1.5721912\n",
      "  0.43281424  1.6291353 ]\n",
      "#leadership 11070 [ 0.804105    1.703703   -0.27683935  1.0883378   1.2486427   0.71987647\n",
      " -1.2214324   0.31226104  0.28527248  0.43703973 -1.8948476  -0.66148883\n",
      "  0.5252311   0.3964793  -0.8544514   1.5619631  -1.4804806   0.9278764\n",
      " -0.8143877   1.1089797 ]\n",
      "#worry 238619 [ 1.659063   -0.08081658 -0.51301146  1.7041371   0.99064165  0.66747195\n",
      " -0.7270127   0.53979534  0.6186053  -0.26956725 -1.8310189  -1.1024348\n",
      " -0.0060048  -1.2603769  -1.9151248   0.394539   -0.64696866  1.3250468\n",
      "  0.07283593  0.45277843]\n"
     ]
    }
   ],
   "source": [
    "# wektoryzacja pierwszego tekstu\n",
    "first_text = train_texts[0][0]\n",
    "for word in fasttext.tokenize(first_text):\n",
    "  print(word, model.get_word_id(word), model.get_word_vector(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NWb5iv7T9ML9"
   },
   "source": [
    "Proszę zwrócić uwagę, że fastText jest w stanie przyporządkować reprezentację wektorową nawet dla takich słów, których model języka nie widział w trakcie uczenia (pierwszy token wejściowego tekstu). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eYtFNcbU9qSR"
   },
   "source": [
    "## Model klasyfikacji tekstu LSTM (2 pkt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SOP5PoY7907f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LSTMClassifier0(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier0, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden[-1]\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_rate=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_x)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        output = self.fc(hidden)\n",
    "        return nn.functional.softmax(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset0(Dataset):\n",
    "    def __init__(self, texts, labels, model):\n",
    "        self.model = model\n",
    "        self.texts = [self.preprocess(text) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return [self.model.get_word_vector(word) for word in fasttext.tokenize(text)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, model):\n",
    "        self.model = model\n",
    "        self.texts = [self.preprocess(text) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return [self.model.get_word_vector(word) for word in fasttext.tokenize(text)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'length': len(self.texts[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'tweeteval/datasets'\n",
    "FT_MODEL_PATH = 'dane/fasttext_tweetmodel_btc_sg_20_en.bin'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n9E-pBUY90LF"
   },
   "source": [
    "## Trenowanie modeli LSTM dla ZJAWISKO_1 i ZJAWISKO_2 (2 pkt.)\n",
    "Należy wybrać 2 z 7 dostępnych podzbiorów z [TweetEval](https://github.com/cardiffnlp/tweeteval) anotowanych następującymi zjawiskami: 1) emocje (emotion), 2) emotikony (emoji), 3) ironia (irony), 4) mowa nienawiści (hate speech), 5) mowa ofensywna (offensive language), 6) wydźwięk (sentiment), 7) nastawienie (stance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1Syr87r2-tSO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_texts_and_labels(base_dataset_path, dataset_name, data_type=\"train\"):\n",
    "    base_path = f'{base_dataset_path}/{dataset_name}'\n",
    "    text_path = f'{base_path}/{data_type}_text.txt'\n",
    "    labels_path = f'{base_path}/{data_type}_labels.txt'\n",
    "    \n",
    "    train_texts = pd.read_csv(text_path, sep='\\t', header=None, names=['text'])\n",
    "    train_labels = pd.read_csv(labels_path, sep='\\t', header=None, names=['label'])\n",
    "    \n",
    "    df = pd.concat([train_texts, train_labels], axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x['length'], reverse=True)\n",
    "    sequences, labels, lengths = zip(*[(item['text'], item['label'], item['length']) for item in batch])\n",
    "    sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return {'text': sequences, 'label': labels, 'length': lengths}\n",
    "\n",
    "\n",
    "def get_dataloader(model_path, base_dataset_path, dataset_name, data_type=\"train\"):\n",
    "    model = fasttext.load_model(model_path)\n",
    "    texts, labels = get_texts_and_labels(base_dataset_path, dataset_name, data_type)\n",
    "\n",
    "    dataset = TextDataset(texts, labels, model)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    class_count = len(set(labels))\n",
    "\n",
    "    return dataloader, model.get_dimension(), class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 282/282 [00:03<00:00, 91.02it/s, loss=0.609]\n",
      "Epoch 2/100: 100%|██████████| 282/282 [00:02<00:00, 95.06it/s, loss=0.552]\n",
      "Epoch 3/100: 100%|██████████| 282/282 [00:03<00:00, 92.89it/s, loss=0.599]\n",
      "Epoch 4/100: 100%|██████████| 282/282 [00:03<00:00, 93.85it/s, loss=0.508]\n",
      "Epoch 5/100: 100%|██████████| 282/282 [00:03<00:00, 88.61it/s, loss=0.534]\n",
      "Epoch 6/100: 100%|██████████| 282/282 [00:03<00:00, 90.89it/s, loss=0.615]\n",
      "Epoch 7/100: 100%|██████████| 282/282 [00:03<00:00, 92.10it/s, loss=0.579]\n",
      "Epoch 8/100: 100%|██████████| 282/282 [00:03<00:00, 89.55it/s, loss=0.468]\n",
      "Epoch 9/100: 100%|██████████| 282/282 [00:03<00:00, 91.01it/s, loss=0.516]\n",
      "Epoch 10/100: 100%|██████████| 282/282 [00:03<00:00, 91.20it/s, loss=0.592]\n",
      "Epoch 11/100: 100%|██████████| 282/282 [00:03<00:00, 91.40it/s, loss=0.549]\n",
      "Epoch 12/100: 100%|██████████| 282/282 [00:03<00:00, 89.07it/s, loss=0.52] \n",
      "Epoch 13/100: 100%|██████████| 282/282 [00:03<00:00, 90.43it/s, loss=0.495]\n",
      "Epoch 14/100: 100%|██████████| 282/282 [00:03<00:00, 90.61it/s, loss=0.535]\n",
      "Epoch 15/100: 100%|██████████| 282/282 [00:03<00:00, 88.99it/s, loss=0.464]\n",
      "Epoch 16/100: 100%|██████████| 282/282 [00:03<00:00, 91.61it/s, loss=0.544] \n",
      "Epoch 17/100: 100%|██████████| 282/282 [00:02<00:00, 102.11it/s, loss=0.538]\n",
      "Epoch 18/100: 100%|██████████| 282/282 [00:02<00:00, 102.82it/s, loss=0.582]\n",
      "Epoch 19/100: 100%|██████████| 282/282 [00:02<00:00, 107.89it/s, loss=0.558]\n",
      "Epoch 20/100: 100%|██████████| 282/282 [00:02<00:00, 128.52it/s, loss=0.543]\n",
      "Epoch 21/100: 100%|██████████| 282/282 [00:02<00:00, 122.93it/s, loss=0.507]\n",
      "Epoch 22/100: 100%|██████████| 282/282 [00:02<00:00, 108.61it/s, loss=0.591]\n",
      "Epoch 23/100: 100%|██████████| 282/282 [00:03<00:00, 87.80it/s, loss=0.546] \n",
      "Epoch 24/100: 100%|██████████| 282/282 [00:02<00:00, 107.57it/s, loss=0.528]\n",
      "Epoch 25/100: 100%|██████████| 282/282 [00:02<00:00, 105.78it/s, loss=0.562]\n",
      "Epoch 26/100: 100%|██████████| 282/282 [00:02<00:00, 107.39it/s, loss=0.567]\n",
      "Epoch 27/100: 100%|██████████| 282/282 [00:02<00:00, 137.63it/s, loss=0.542]\n",
      "Epoch 28/100: 100%|██████████| 282/282 [00:02<00:00, 137.32it/s, loss=0.518]\n",
      "Epoch 29/100: 100%|██████████| 282/282 [00:02<00:00, 136.14it/s, loss=0.514]\n",
      "Epoch 30/100: 100%|██████████| 282/282 [00:01<00:00, 144.11it/s, loss=0.555]\n",
      "Epoch 31/100: 100%|██████████| 282/282 [00:01<00:00, 142.72it/s, loss=0.474]\n",
      "Epoch 32/100: 100%|██████████| 282/282 [00:02<00:00, 135.87it/s, loss=0.557]\n",
      "Epoch 33/100: 100%|██████████| 282/282 [00:02<00:00, 135.51it/s, loss=0.492]\n",
      "Epoch 34/100: 100%|██████████| 282/282 [00:02<00:00, 139.21it/s, loss=0.563]\n",
      "Epoch 35/100: 100%|██████████| 282/282 [00:01<00:00, 142.35it/s, loss=0.514]\n",
      "Epoch 36/100: 100%|██████████| 282/282 [00:01<00:00, 143.11it/s, loss=0.552]\n",
      "Epoch 37/100: 100%|██████████| 282/282 [00:02<00:00, 138.60it/s, loss=0.499]\n",
      "Epoch 38/100: 100%|██████████| 282/282 [00:02<00:00, 135.18it/s, loss=0.519]\n",
      "Epoch 39/100: 100%|██████████| 282/282 [00:02<00:00, 137.66it/s, loss=0.567]\n",
      "Epoch 40/100: 100%|██████████| 282/282 [00:01<00:00, 142.89it/s, loss=0.542]\n",
      "Epoch 41/100: 100%|██████████| 282/282 [00:02<00:00, 139.04it/s, loss=0.568]\n",
      "Epoch 42/100: 100%|██████████| 282/282 [00:01<00:00, 141.35it/s, loss=0.494]\n",
      "Epoch 43/100: 100%|██████████| 282/282 [00:02<00:00, 139.02it/s, loss=0.493]\n",
      "Epoch 44/100: 100%|██████████| 282/282 [00:02<00:00, 111.40it/s, loss=0.438]\n",
      "Epoch 45/100: 100%|██████████| 282/282 [00:02<00:00, 108.08it/s, loss=0.503]\n",
      "Epoch 46/100: 100%|██████████| 282/282 [00:02<00:00, 97.98it/s, loss=0.54]  \n",
      "Epoch 47/100: 100%|██████████| 282/282 [00:03<00:00, 90.41it/s, loss=0.563]\n",
      "Epoch 48/100: 100%|██████████| 282/282 [00:02<00:00, 100.07it/s, loss=0.472]\n",
      "Epoch 49/100: 100%|██████████| 282/282 [00:02<00:00, 103.21it/s, loss=0.503]\n",
      "Epoch 50/100: 100%|██████████| 282/282 [00:02<00:00, 94.49it/s, loss=0.521] \n",
      "Epoch 51/100: 100%|██████████| 282/282 [00:02<00:00, 117.42it/s, loss=0.552]\n",
      "Epoch 52/100: 100%|██████████| 282/282 [00:02<00:00, 140.75it/s, loss=0.488]\n",
      "Epoch 53/100: 100%|██████████| 282/282 [00:02<00:00, 136.22it/s, loss=0.495]\n",
      "Epoch 54/100: 100%|██████████| 282/282 [00:02<00:00, 108.13it/s, loss=0.529]\n",
      "Epoch 55/100: 100%|██████████| 282/282 [00:02<00:00, 95.08it/s, loss=0.431]\n",
      "Epoch 56/100: 100%|██████████| 282/282 [00:02<00:00, 105.07it/s, loss=0.449]\n",
      "Epoch 57/100: 100%|██████████| 282/282 [00:02<00:00, 102.04it/s, loss=0.504]\n",
      "Epoch 58/100: 100%|██████████| 282/282 [00:02<00:00, 113.61it/s, loss=0.554]\n",
      "Epoch 59/100: 100%|██████████| 282/282 [00:02<00:00, 135.58it/s, loss=0.504]\n",
      "Epoch 60/100: 100%|██████████| 282/282 [00:01<00:00, 142.32it/s, loss=0.447]\n",
      "Epoch 61/100: 100%|██████████| 282/282 [00:02<00:00, 123.74it/s, loss=0.492]\n",
      "Epoch 62/100: 100%|██████████| 282/282 [00:02<00:00, 116.45it/s, loss=0.448]\n",
      "Epoch 63/100: 100%|██████████| 282/282 [00:02<00:00, 138.27it/s, loss=0.481]\n",
      "Epoch 64/100: 100%|██████████| 282/282 [00:02<00:00, 137.00it/s, loss=0.554]\n",
      "Epoch 65/100: 100%|██████████| 282/282 [00:02<00:00, 125.12it/s, loss=0.445]\n",
      "Epoch 66/100: 100%|██████████| 282/282 [00:02<00:00, 100.47it/s, loss=0.511]\n",
      "Epoch 67/100: 100%|██████████| 282/282 [00:03<00:00, 88.21it/s, loss=0.474]\n",
      "Epoch 68/100: 100%|██████████| 282/282 [00:02<00:00, 95.40it/s, loss=0.453] \n",
      "Epoch 69/100: 100%|██████████| 282/282 [00:02<00:00, 99.21it/s, loss=0.551] \n",
      "Epoch 70/100: 100%|██████████| 282/282 [00:02<00:00, 113.10it/s, loss=0.47] \n",
      "Epoch 71/100: 100%|██████████| 282/282 [00:02<00:00, 137.96it/s, loss=0.467]\n",
      "Epoch 72/100: 100%|██████████| 282/282 [00:02<00:00, 133.18it/s, loss=0.453]\n",
      "Epoch 73/100: 100%|██████████| 282/282 [00:02<00:00, 114.47it/s, loss=0.554]\n",
      "Epoch 74/100: 100%|██████████| 282/282 [00:02<00:00, 97.16it/s, loss=0.422] \n",
      "Epoch 75/100: 100%|██████████| 282/282 [00:02<00:00, 102.81it/s, loss=0.48] \n",
      "Epoch 76/100: 100%|██████████| 282/282 [00:02<00:00, 103.65it/s, loss=0.488]\n",
      "Epoch 77/100: 100%|██████████| 282/282 [00:02<00:00, 101.04it/s, loss=0.445]\n",
      "Epoch 78/100: 100%|██████████| 282/282 [00:03<00:00, 93.85it/s, loss=0.441] \n",
      "Epoch 79/100: 100%|██████████| 282/282 [00:03<00:00, 90.53it/s, loss=0.447]\n",
      "Epoch 80/100: 100%|██████████| 282/282 [00:02<00:00, 115.32it/s, loss=0.547]\n",
      "Epoch 81/100: 100%|██████████| 282/282 [00:02<00:00, 136.17it/s, loss=0.452]\n",
      "Epoch 82/100: 100%|██████████| 282/282 [00:02<00:00, 137.70it/s, loss=0.48] \n",
      "Epoch 83/100: 100%|██████████| 282/282 [00:02<00:00, 138.10it/s, loss=0.455]\n",
      "Epoch 84/100: 100%|██████████| 282/282 [00:01<00:00, 142.14it/s, loss=0.48] \n",
      "Epoch 85/100: 100%|██████████| 282/282 [00:01<00:00, 141.65it/s, loss=0.443]\n",
      "Epoch 86/100: 100%|██████████| 282/282 [00:02<00:00, 138.32it/s, loss=0.44] \n",
      "Epoch 87/100: 100%|██████████| 282/282 [00:02<00:00, 138.30it/s, loss=0.395]\n",
      "Epoch 88/100: 100%|██████████| 282/282 [00:02<00:00, 140.54it/s, loss=0.459]\n",
      "Epoch 89/100: 100%|██████████| 282/282 [00:02<00:00, 139.39it/s, loss=0.414]\n",
      "Epoch 90/100: 100%|██████████| 282/282 [00:01<00:00, 142.53it/s, loss=0.466]\n",
      "Epoch 91/100: 100%|██████████| 282/282 [00:02<00:00, 138.72it/s, loss=0.467]\n",
      "Epoch 92/100: 100%|██████████| 282/282 [00:02<00:00, 136.89it/s, loss=0.402]\n",
      "Epoch 93/100: 100%|██████████| 282/282 [00:02<00:00, 135.19it/s, loss=0.38] \n",
      "Epoch 94/100: 100%|██████████| 282/282 [00:02<00:00, 114.33it/s, loss=0.398]\n",
      "Epoch 95/100: 100%|██████████| 282/282 [00:02<00:00, 104.79it/s, loss=0.43] \n",
      "Epoch 96/100: 100%|██████████| 282/282 [00:02<00:00, 127.10it/s, loss=0.423]\n",
      "Epoch 97/100: 100%|██████████| 282/282 [00:02<00:00, 138.57it/s, loss=0.406]\n",
      "Epoch 98/100: 100%|██████████| 282/282 [00:02<00:00, 129.86it/s, loss=0.459]\n",
      "Epoch 99/100: 100%|██████████| 282/282 [00:02<00:00, 109.70it/s, loss=0.437]\n",
      "Epoch 100/100: 100%|██████████| 282/282 [00:02<00:00, 105.13it/s, loss=0.429]\n",
      "Epoch 1/100: 100%|██████████| 373/373 [00:03<00:00, 106.45it/s, loss=0.63] \n",
      "Epoch 2/100: 100%|██████████| 373/373 [00:04<00:00, 87.63it/s, loss=0.647]\n",
      "Epoch 3/100: 100%|██████████| 373/373 [00:04<00:00, 81.92it/s, loss=0.645]\n",
      "Epoch 4/100: 100%|██████████| 373/373 [00:04<00:00, 88.50it/s, loss=0.624]\n",
      "Epoch 5/100: 100%|██████████| 373/373 [00:04<00:00, 82.62it/s, loss=0.598]\n",
      "Epoch 6/100: 100%|██████████| 373/373 [00:04<00:00, 88.34it/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 373/373 [00:04<00:00, 86.07it/s, loss=0.632]\n",
      "Epoch 8/100: 100%|██████████| 373/373 [00:04<00:00, 84.80it/s, loss=0.644]\n",
      "Epoch 9/100: 100%|██████████| 373/373 [00:04<00:00, 88.19it/s, loss=0.654]\n",
      "Epoch 10/100: 100%|██████████| 373/373 [00:04<00:00, 90.43it/s, loss=0.613]\n",
      "Epoch 11/100: 100%|██████████| 373/373 [00:04<00:00, 86.64it/s, loss=0.678]\n",
      "Epoch 12/100: 100%|██████████| 373/373 [00:04<00:00, 83.78it/s, loss=0.623]\n",
      "Epoch 13/100: 100%|██████████| 373/373 [00:04<00:00, 87.48it/s, loss=0.64] \n",
      "Epoch 14/100: 100%|██████████| 373/373 [00:03<00:00, 99.41it/s, loss=0.662] \n",
      "Epoch 15/100: 100%|██████████| 373/373 [00:03<00:00, 116.11it/s, loss=0.703]\n",
      "Epoch 16/100: 100%|██████████| 373/373 [00:03<00:00, 115.48it/s, loss=0.647]\n",
      "Epoch 17/100: 100%|██████████| 373/373 [00:03<00:00, 118.60it/s, loss=0.644]\n",
      "Epoch 18/100: 100%|██████████| 373/373 [00:03<00:00, 117.71it/s, loss=0.692]\n",
      "Epoch 19/100: 100%|██████████| 373/373 [00:03<00:00, 117.04it/s, loss=0.718]\n",
      "Epoch 20/100: 100%|██████████| 373/373 [00:03<00:00, 111.94it/s, loss=0.709]\n",
      "Epoch 21/100: 100%|██████████| 373/373 [00:03<00:00, 111.34it/s, loss=0.654]\n",
      "Epoch 22/100: 100%|██████████| 373/373 [00:03<00:00, 116.47it/s, loss=0.684]\n",
      "Epoch 23/100: 100%|██████████| 373/373 [00:03<00:00, 114.15it/s, loss=0.683]\n",
      "Epoch 24/100: 100%|██████████| 373/373 [00:03<00:00, 112.52it/s, loss=0.668]\n",
      "Epoch 25/100: 100%|██████████| 373/373 [00:03<00:00, 96.55it/s, loss=0.715]\n",
      "Epoch 26/100: 100%|██████████| 373/373 [00:04<00:00, 87.61it/s, loss=0.693]\n",
      "Epoch 27/100: 100%|██████████| 373/373 [00:04<00:00, 84.68it/s, loss=0.713]\n",
      "Epoch 28/100: 100%|██████████| 373/373 [00:04<00:00, 87.90it/s, loss=0.645]\n",
      "Epoch 29/100: 100%|██████████| 373/373 [00:04<00:00, 82.54it/s, loss=0.71] \n",
      "Epoch 30/100: 100%|██████████| 373/373 [00:04<00:00, 87.79it/s, loss=0.649]\n",
      "Epoch 31/100: 100%|██████████| 373/373 [00:04<00:00, 83.45it/s, loss=0.735]\n",
      "Epoch 32/100: 100%|██████████| 373/373 [00:04<00:00, 85.41it/s, loss=0.677]\n",
      "Epoch 33/100: 100%|██████████| 373/373 [00:04<00:00, 86.97it/s, loss=0.661]\n",
      "Epoch 34/100: 100%|██████████| 373/373 [00:03<00:00, 100.16it/s, loss=0.732]\n",
      "Epoch 35/100: 100%|██████████| 373/373 [00:03<00:00, 113.17it/s, loss=0.658]\n",
      "Epoch 36/100: 100%|██████████| 373/373 [00:03<00:00, 106.22it/s, loss=0.675]\n",
      "Epoch 37/100: 100%|██████████| 373/373 [00:03<00:00, 111.99it/s, loss=0.67] \n",
      "Epoch 38/100: 100%|██████████| 373/373 [00:03<00:00, 94.46it/s, loss=0.691] \n",
      "Epoch 39/100: 100%|██████████| 373/373 [00:04<00:00, 86.42it/s, loss=0.661]\n",
      "Epoch 40/100: 100%|██████████| 373/373 [00:04<00:00, 86.84it/s, loss=0.69] \n",
      "Epoch 41/100: 100%|██████████| 373/373 [00:04<00:00, 89.13it/s, loss=0.704]\n",
      "Epoch 42/100: 100%|██████████| 373/373 [00:04<00:00, 90.61it/s, loss=0.679]\n",
      "Epoch 43/100: 100%|██████████| 373/373 [00:03<00:00, 94.60it/s, loss=0.674]\n",
      "Epoch 44/100: 100%|██████████| 373/373 [00:04<00:00, 93.06it/s, loss=0.702]\n",
      "Epoch 45/100: 100%|██████████| 373/373 [00:03<00:00, 93.94it/s, loss=0.678]\n",
      "Epoch 46/100: 100%|██████████| 373/373 [00:04<00:00, 92.28it/s, loss=0.677]\n",
      "Epoch 47/100: 100%|██████████| 373/373 [00:04<00:00, 90.67it/s, loss=0.66] \n",
      "Epoch 48/100: 100%|██████████| 373/373 [00:04<00:00, 92.47it/s, loss=0.697]\n",
      "Epoch 49/100: 100%|██████████| 373/373 [00:03<00:00, 111.96it/s, loss=0.681]\n",
      "Epoch 50/100: 100%|██████████| 373/373 [00:03<00:00, 115.23it/s, loss=0.68] \n",
      "Epoch 51/100: 100%|██████████| 373/373 [00:03<00:00, 112.04it/s, loss=0.656]\n",
      "Epoch 52/100: 100%|██████████| 373/373 [00:04<00:00, 84.08it/s, loss=0.678]\n",
      "Epoch 53/100: 100%|██████████| 373/373 [00:04<00:00, 89.96it/s, loss=0.692]\n",
      "Epoch 54/100: 100%|██████████| 373/373 [00:04<00:00, 84.55it/s, loss=0.647]\n",
      "Epoch 55/100: 100%|██████████| 373/373 [00:04<00:00, 81.33it/s, loss=0.665]\n",
      "Epoch 56/100: 100%|██████████| 373/373 [00:04<00:00, 88.29it/s, loss=0.679]\n",
      "Epoch 57/100: 100%|██████████| 373/373 [00:04<00:00, 90.53it/s, loss=0.671]\n",
      "Epoch 58/100: 100%|██████████| 373/373 [00:04<00:00, 86.15it/s, loss=0.688]\n",
      "Epoch 59/100: 100%|██████████| 373/373 [00:04<00:00, 88.32it/s, loss=0.685]\n",
      "Epoch 60/100: 100%|██████████| 373/373 [00:04<00:00, 88.88it/s, loss=0.712]\n",
      "Epoch 61/100: 100%|██████████| 373/373 [00:04<00:00, 85.80it/s, loss=0.679]\n",
      "Epoch 62/100: 100%|██████████| 373/373 [00:03<00:00, 96.70it/s, loss=0.711] \n",
      "Epoch 63/100: 100%|██████████| 373/373 [00:03<00:00, 113.96it/s, loss=0.68] \n",
      "Epoch 64/100: 100%|██████████| 373/373 [00:03<00:00, 95.35it/s, loss=0.684] \n",
      "Epoch 65/100: 100%|██████████| 373/373 [00:04<00:00, 89.57it/s, loss=0.718]\n",
      "Epoch 66/100: 100%|██████████| 373/373 [00:04<00:00, 89.74it/s, loss=0.656]\n",
      "Epoch 67/100: 100%|██████████| 373/373 [00:04<00:00, 89.36it/s, loss=0.672]\n",
      "Epoch 68/100: 100%|██████████| 373/373 [00:04<00:00, 90.40it/s, loss=0.685]\n",
      "Epoch 69/100: 100%|██████████| 373/373 [00:04<00:00, 88.74it/s, loss=0.698]\n",
      "Epoch 70/100: 100%|██████████| 373/373 [00:04<00:00, 89.88it/s, loss=0.711]\n",
      "Epoch 71/100: 100%|██████████| 373/373 [00:04<00:00, 90.26it/s, loss=0.743]\n",
      "Epoch 72/100: 100%|██████████| 373/373 [00:04<00:00, 88.88it/s, loss=0.718]\n",
      "Epoch 73/100: 100%|██████████| 373/373 [00:04<00:00, 87.24it/s, loss=0.706]\n",
      "Epoch 74/100: 100%|██████████| 373/373 [00:04<00:00, 86.54it/s, loss=0.721]\n",
      "Epoch 75/100: 100%|██████████| 373/373 [00:04<00:00, 80.15it/s, loss=0.716]\n",
      "Epoch 76/100: 100%|██████████| 373/373 [00:04<00:00, 87.19it/s, loss=0.709]\n",
      "Epoch 77/100: 100%|██████████| 373/373 [00:04<00:00, 89.65it/s, loss=0.711]\n",
      "Epoch 78/100: 100%|██████████| 373/373 [00:04<00:00, 89.67it/s, loss=0.73] \n",
      "Epoch 79/100: 100%|██████████| 373/373 [00:04<00:00, 89.77it/s, loss=0.702]\n",
      "Epoch 80/100: 100%|██████████| 373/373 [00:04<00:00, 90.64it/s, loss=0.688]\n",
      "Epoch 81/100: 100%|██████████| 373/373 [00:04<00:00, 92.39it/s, loss=0.746] \n",
      "Epoch 82/100: 100%|██████████| 373/373 [00:03<00:00, 108.31it/s, loss=0.748]\n",
      "Epoch 83/100: 100%|██████████| 373/373 [00:03<00:00, 112.70it/s, loss=0.714]\n",
      "Epoch 84/100: 100%|██████████| 373/373 [00:03<00:00, 113.23it/s, loss=0.718]\n",
      "Epoch 85/100: 100%|██████████| 373/373 [00:03<00:00, 118.07it/s, loss=0.735]\n",
      "Epoch 86/100: 100%|██████████| 373/373 [00:03<00:00, 114.12it/s, loss=0.69] \n",
      "Epoch 87/100: 100%|██████████| 373/373 [00:04<00:00, 89.93it/s, loss=0.75] \n",
      "Epoch 88/100: 100%|██████████| 373/373 [00:04<00:00, 89.78it/s, loss=0.751] \n",
      "Epoch 89/100: 100%|██████████| 373/373 [00:03<00:00, 118.88it/s, loss=0.71] \n",
      "Epoch 90/100: 100%|██████████| 373/373 [00:03<00:00, 115.41it/s, loss=0.711]\n",
      "Epoch 91/100: 100%|██████████| 373/373 [00:03<00:00, 119.48it/s, loss=0.72] \n",
      "Epoch 92/100: 100%|██████████| 373/373 [00:03<00:00, 120.08it/s, loss=0.702]\n",
      "Epoch 93/100: 100%|██████████| 373/373 [00:03<00:00, 114.76it/s, loss=0.736]\n",
      "Epoch 94/100: 100%|██████████| 373/373 [00:03<00:00, 120.60it/s, loss=0.723]\n",
      "Epoch 95/100: 100%|██████████| 373/373 [00:03<00:00, 116.94it/s, loss=0.727]\n",
      "Epoch 96/100: 100%|██████████| 373/373 [00:03<00:00, 119.98it/s, loss=0.725]\n",
      "Epoch 97/100: 100%|██████████| 373/373 [00:03<00:00, 120.22it/s, loss=0.74] \n",
      "Epoch 98/100: 100%|██████████| 373/373 [00:03<00:00, 118.46it/s, loss=0.75] \n",
      "Epoch 99/100: 100%|██████████| 373/373 [00:03<00:00, 114.54it/s, loss=0.759]\n",
      "Epoch 100/100: 100%|██████████| 373/373 [00:03<00:00, 118.32it/s, loss=0.722]\n"
     ]
    }
   ],
   "source": [
    "def train_lstm_model(emotion, num_epochs=100, hidden_dim=50, dropout_rate=0.7):\n",
    "    dataloader, embedding_dim, class_count = get_dataloader(FT_MODEL_PATH, BASE_PATH, emotion)\n",
    "\n",
    "    hidden_dim = hidden_dim # 100-500 => tuning\n",
    "    output_dim = class_count\n",
    "    lstm_model = LSTMClassifier(embedding_dim, hidden_dim, output_dim, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    print(embedding_dim, class_count)\n",
    "    return\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(dataloader, total=len(dataloader), leave=True)\n",
    "        for batch in loop:\n",
    "            texts = batch['text'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            lengths = batch['length']\n",
    "\n",
    "            outputs = lstm_model(texts, lengths)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "hate_lstm_model = train_lstm_model(\"hate\")\n",
    "offensive_lstm_model = train_lstm_model(\"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hate_lstm_model.state_dict(), 'hate_lstm_model.pt')\n",
    "torch.save(offensive_lstm_model.state_dict(), 'offensive_lstm_model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bvy1SbHQ-lFs"
   },
   "source": [
    "## Trenowanie modeli LR (fastText) dla ZJAWISKO_1 i ZJAWISKO_2 (2 pkt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "0V4g2jmv-weV"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    if pd.isna(text):\n",
    "        return np.zeros(model.get_dimension())\n",
    "\n",
    "    word_vectors = [model.get_word_vector(word) for word in fasttext.tokenize(text)]\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "def get_avg_vector(texts, model):\n",
    "    def text_to_avg_vector(text, model):\n",
    "        word_vectors = vectorize_text(text, model)\n",
    "        return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.get_dimension())\n",
    "\n",
    "    avg_word_vectors = np.array([text_to_avg_vector(text, model) for text in texts])\n",
    "    return avg_word_vectors\n",
    "\n",
    "\n",
    "def scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "def predict(\n",
    "    classifier,\n",
    "    X_Train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "):\n",
    "    classifier.fit(X_Train, y_train)\n",
    "\n",
    "    y_pred_train = classifier.predict(X_Train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    return y_pred_train, y_pred_test\n",
    "\n",
    "def train_lf_model(emotion):  \n",
    "    model = fasttext.load_model(FT_MODEL_PATH)\n",
    "    \n",
    "    text_train, y_train = get_texts_and_labels(BASE_PATH, emotion, \"train\")\n",
    "    text_test, y_test = get_texts_and_labels(BASE_PATH, emotion, \"test\")\n",
    "\n",
    "    X_train = get_avg_vector(text_train, model)\n",
    "    X_test = get_avg_vector(text_test, model)\n",
    "\n",
    "    X_train, X_test = scale(X_train, X_test)\n",
    "    \n",
    "    classifier = LogisticRegression(multi_class='ovr')\n",
    "    y_pred_train, y_pred_test = predict(classifier, X_train, X_test, y_train)\n",
    "    \n",
    "    return classifier, X_test, y_test, y_pred_test\n",
    "\n",
    "    \n",
    "hate_lr_model, X_test_hate, y_test_hate, y_pred_test_hate = train_lf_model(\"hate\")\n",
    "offensive_lr_model, X_test_offensive, y_test_offensive, y_pred_test_offensive = train_lf_model(\"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "def classification_metrics_lr(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f'Accuracy LR :{acc}')\n",
    "    print(f'Confusion Matrix LR: \\n{cm}')\n",
    "    print(f'Classification Report LR: \\n{report}')\n",
    "    \n",
    "def classification_metrics_lstm(model, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing\", unit=\"batch\"):\n",
    "            inputs = batch['text'].float().to(device)\n",
    "            lengths = batch['length']\n",
    "            outputs = model(inputs, lengths)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calculate_accuracy_lstm(predictions, labels):\n",
    "    correct = sum(p == l for p, l in zip(predictions, labels))\n",
    "    accuracy = correct / len(labels) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8neBnDzU-4o9"
   },
   "source": [
    "## Ewaluacja modeli na danych testowych dla zjawiska ZJAWISKO_1 (2 pkt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "7wkbdG93_FxE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LR :0.5576398362892224\n",
      "Confusion Matrix LR: \n",
      "[[1522  175]\n",
      " [1122  113]]\n",
      "Classification Report LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.90      0.70      1697\n",
      "           1       0.39      0.09      0.15      1235\n",
      "\n",
      "    accuracy                           0.56      2932\n",
      "   macro avg       0.48      0.49      0.42      2932\n",
      "weighted avg       0.50      0.56      0.47      2932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 92/92 [00:00<00:00, 180.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LSTM: 57.094133697135064%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classification_metrics_lr(y_test_hate, y_pred_test_hate)\n",
    "\n",
    "\n",
    "train_dataloader, embedding_dim, class_count = get_dataloader(FT_MODEL_PATH, BASE_PATH, 'hate', 'test')\n",
    "predictions = classification_metrics_lstm(hate_lstm_model, train_dataloader)\n",
    "\n",
    "accuracy_lstm = calculate_accuracy_lstm(predictions, y_test_hate)\n",
    "print(f\"Accuracy LSTM: {accuracy_lstm}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qIHyGqMM_HaE"
   },
   "source": [
    "## Ewaluacja modeli na danych testowych dla zjawiska ZJAWISKO_2 (2 pkt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "MQ8yTGuu_Ird"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LR :0.75\n",
      "Confusion Matrix LR: \n",
      "[[597  23]\n",
      " [192  48]]\n",
      "Classification Report LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       620\n",
      "           1       0.68      0.20      0.31       240\n",
      "\n",
      "    accuracy                           0.75       860\n",
      "   macro avg       0.72      0.58      0.58       860\n",
      "weighted avg       0.73      0.75      0.70       860\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 27/27 [00:00<00:00, 166.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LSTM: 66.97674418604652%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classification_metrics_lr(y_test_offensive, y_pred_test_offensive)\n",
    "\n",
    "\n",
    "train_dataloader, embedding_dim, class_count = get_dataloader(FT_MODEL_PATH, BASE_PATH, 'offensive', 'test')\n",
    "predictions = classification_metrics_lstm(offensive_lstm_model, train_dataloader)\n",
    "\n",
    "accuracy_lstm = calculate_accuracy_lstm(predictions, y_test_offensive)\n",
    "print(f\"Accuracy LSTM: {accuracy_lstm}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
