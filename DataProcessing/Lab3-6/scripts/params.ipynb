{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from Review import Review\n",
    "from sklearn.svm import SVC\n",
    "from train_model import train\n",
    "import matplotlib.pyplot as plt\n",
    "from split_data import split_data\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from preprocess_data import preprocess_data\n",
    "from extract_features import extract_features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    "    HalvingRandomSearchCV,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomClassifier(BaseEstimator, TransformerMixin):\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_feature: str,\n",
    "        files: List[str],\n",
    "        vectorizer_method: str,\n",
    "        vectorizer_length: int,\n",
    "        best_features: int,\n",
    "        pca_comps: int,\n",
    "        classifier_name: str,\n",
    "    ):\n",
    "        self.files = files\n",
    "        self.target_feature = target_feature\n",
    "        self.classifier_name = classifier_name\n",
    "        self.vectorizer_method = vectorizer_method\n",
    "        self.vectorizer_length = vectorizer_length\n",
    "        self.best_features = best_features\n",
    "        self.pca_comps = pca_comps\n",
    "\n",
    "        self._train_df = None\n",
    "        self._test_df = None\n",
    "        self._y_pred_train = None\n",
    "        self._y_pred_test = None\n",
    "        self._y_train = None\n",
    "        self._y_test = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        train_df, test_df = split_data(X, self.target_feature)\n",
    "        train_df, test_df = extract_features(\n",
    "            train_df,\n",
    "            test_df,\n",
    "            self.target_feature,\n",
    "            vectorizer_method=self.vectorizer_method,\n",
    "            vectorizer_length=self.vectorizer_length,\n",
    "        )\n",
    "\n",
    "        y_pred_train, y_pred_test, y_train, y_test = train(\n",
    "            self._get_classifier(),\n",
    "            train_df,\n",
    "            test_df,\n",
    "            self.target_feature,\n",
    "            n_best_features=self.best_features,\n",
    "            n_pca_comps=self.pca_comps,\n",
    "        )\n",
    "        self._train_df = train_df\n",
    "        self._test_df = test_df\n",
    "        self._y_pred_train = y_pred_train\n",
    "        self._y_pred_test = y_pred_test\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self._test_df\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        return self._y_pred_test\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return accuracy_score(self._y_test, self._y_pred_test)\n",
    "\n",
    "    def _get_classifier(self):\n",
    "        if self.classifier_name == \"dummy\":\n",
    "            return DummyClassifier(strategy=\"uniform\", random_state=self._seed)\n",
    "        elif self.classifier_name == \"SVC\":\n",
    "            return SVC(random_state=self._seed)\n",
    "        elif self.classifier_name == \"RandomForest\":\n",
    "            return RandomForestClassifier(random_state=self._seed)\n",
    "        else:\n",
    "            raise ValueError(f\"{self.classifier_name} not supported as classifier\")\n",
    "\n",
    "\n",
    "files = glob.glob(\"../data/input/*.json\")\n",
    "seed = 42\n",
    "\n",
    "classifiers = [\"SVC\", \"RandomForest\"]\n",
    "vectorizers = [\"bag-of-words\", \"tf-idf\", \"word2vec\"]\n",
    "vectorizers_lenghts = [500, 750, 1000]\n",
    "n_best_features = [200]\n",
    "n_pca_comps = [50]\n",
    "\n",
    "\n",
    "# classifiers = [\"SVC\"]\n",
    "# vectorizers = [\"bag-of-words\"]\n",
    "# vectorizers_lenghts = [500]\n",
    "# n_best_features = [50]\n",
    "# n_pca_comps = [10, 30]\n",
    "\n",
    "param_grid = {\n",
    "    \"vectorizer_method\": vectorizers,\n",
    "    \"vectorizer_length\": vectorizers_lenghts,\n",
    "    \"best_features\": n_best_features,\n",
    "    \"pca_comps\": n_pca_comps,\n",
    "    \"classifier_name\": classifiers,\n",
    "}\n",
    "\n",
    "\n",
    "def count_grid_combinations(param_grid):\n",
    "    n_combinations = 1\n",
    "    for key, values in param_grid.items():\n",
    "        n_combinations *= len(values)\n",
    "    return n_combinations\n",
    "\n",
    "\n",
    "def estimate_iterations(search_class, search, param_grid, n_iter=None):\n",
    "    if search_class == GridSearchCV:\n",
    "        return count_grid_combinations(param_grid)\n",
    "    elif search_class == RandomizedSearchCV:\n",
    "        if n_iter is not None:\n",
    "            return n_iter\n",
    "        else:\n",
    "            return min(10, count_grid_combinations(param_grid))  # 10 is default value\n",
    "    elif getattr(search, \"n_iterations_\", None) is not None:\n",
    "        return getattr(search, \"n_iterations_\", None)\n",
    "    else:\n",
    "        raise ValueError(f\"could not estimate iterations {search_class}\")\n",
    "\n",
    "\n",
    "def messure_params_search_method(search_class, n_iter=None):\n",
    "    clf = CustomClassifier(\n",
    "        target_feature=Review.target_feature,\n",
    "        files=files,\n",
    "        vectorizer_method=vectorizers[0],\n",
    "        vectorizer_length=vectorizers_lenghts[0],\n",
    "        best_features=n_best_features[0],\n",
    "        pca_comps=n_pca_comps[0],\n",
    "        classifier_name=classifiers[0],\n",
    "    )\n",
    "    reviews_df = preprocess_data(files)\n",
    "    search = None\n",
    "    if n_iter is not None and search_class == RandomizedSearchCV:\n",
    "        search = search_class(clf, param_grid, n_jobs=1, verbose=1, n_iter=n_iter)\n",
    "    else:\n",
    "        search = search_class(clf, param_grid, n_jobs=1, verbose=1)\n",
    "    start_time = time.time()\n",
    "    search.fit(reviews_df)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    score = search.best_score_\n",
    "    params = search.best_params_\n",
    "    num_iterations = estimate_iterations(search_class, search, param_grid)\n",
    "    max_iterations = getattr(search, \"n_possible_iterations_\", num_iterations)\n",
    "\n",
    "    print(f\"{search_class.__name__}:\")\n",
    "    print(\"Best score:\", score)\n",
    "    print(\"Time taken(s):\", elapsed_time)\n",
    "    print(\"Iterations:\", num_iterations)\n",
    "    print(\"Max possible iterations\", max_iterations)\n",
    "    print(\"Best params\", params)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return search_class.__name__, score, elapsed_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomSearch - metoda polegająca na losowym przeszukiwaniu przestrzeni hiperparametrów. W każdej iteracji losowane są wartości hiperparametrów z określonego przedziału i tworzony jest model z takimi parametrami. Możliwości: losowy charakter pozwala na szybkie przeszukanie dużej przestrzeni hiperparametrów. Ograniczenia: może przegapić najlepsze ustawienia hiperparametrów.\n",
    "\n",
    "GridSearch - metoda polegająca na przeszukaniu siatki punktów w przestrzeni hiperparametrów. Tworzone są modele dla każdej kombinacji hiperparametrów zdefiniowanych w siatce. Możliwości: dokładne przeszukanie wszystkich kombinacji hiperparametrów. Ograniczenia: może być bardzo kosztowne obliczeniowo, szczególnie dla dużej przestrzeni hiperparametrów.\n",
    "\n",
    "HalvingGridSearch - metoda polegająca na przeszukaniu siatki punktów w przestrzeni hiperparametrów, ale z wykorzystaniem strategii dzielenia i podbierania próbek (ang. halving). W każdej iteracji losowana jest połowa punktów z siatki, a następnie tworzone są modele z użyciem tych punktów. Następnie wybierana jest najlepsza połowa z tych modeli i proces jest powtarzany aż do uzyskania najlepszego zestawu hiperparametrów. Możliwości: szybkie przeszukanie przestrzeni hiperparametrów w porównaniu do GridSearch. Ograniczenia: może przegapić najlepsze ustawienia hiperparametrów.\n",
    "\n",
    "HalvingRandomSearch - metoda polegająca na losowym przeszukiwaniu przestrzeni hiperparametrów z wykorzystaniem strategii dzielenia i podbierania próbek. W każdej iteracji losowana jest połowa punktów z przestrzeni hiperparametrów, a następnie tworzone są modele z użyciem tych punktów. Następnie wybierana jest najlepsza połowa z tych modeli i proces jest powtarzany aż do uzyskania najlepszego zestawu hiperparametrów. Możliwości: szybkie przeszukanie przestrzeni hiperparametrów w porównaniu do RandomSearch. Ograniczenia: może przegapić najlepsze ustawienia hiperparametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "best_scores = []\n",
    "elapsed_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, best_score, elapsed_time = messure_params_search_method(RandomizedSearchCV)\n",
    "methods.append(name)\n",
    "best_scores.append(best_score)\n",
    "elapsed_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, best_score, elapsed_time = messure_params_search_method(GridSearchCV)\n",
    "methods.append(name)\n",
    "best_scores.append(best_score)\n",
    "elapsed_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, best_score, elapsed_time = messure_params_search_method(HalvingGridSearchCV)\n",
    "methods.append(name)\n",
    "best_scores.append(best_score)\n",
    "elapsed_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name, best_score, elapsed_time = messure_params_search_method(HalvingRandomSearchCV)\n",
    "# methods.append(name)\n",
    "# best_scores.append(best_score)\n",
    "# elapsed_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(methods, best_scores)\n",
    "plt.xlabel(\"Methods of hyperparameter optimization\")\n",
    "plt.ylabel(\"Best result (F1 score)\")\n",
    "plt.title(\"Quality of the best model\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(methods, elapsed_times)\n",
    "plt.xlabel(\"Methods of hyperparameter optimization\")\n",
    "plt.ylabel(\"Execution time (s)\")\n",
    "plt.title(\"Execution time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względu na użycie selekcji cech oraz PCA które w dużym stopniu redukują potrzebe ustawiania dużych wartośći vectorizers_lenghts = [500, 750, 1000] uznaje ,że HalvingGridSearchCV jest najlepszym wyborem w moim przypadku. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "best_scores = []\n",
    "elapsed_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [SVC, RandomForestClassifier]\n",
    "vectorizers = [\"bag-of-words\", \"tf-idf\", \"word2vec\"]\n",
    "\n",
    "for classifier_class in classifiers:\n",
    "    for vectorizer in vectorizers:\n",
    "        reviews_df = preprocess_data(files)\n",
    "        train_df, test_df = split_data(reviews_df, Review.target_feature)\n",
    "        start_time = time.time()\n",
    "        train_df, test_df = extract_features(\n",
    "            train_df, test_df, Review.target_feature, vectorizer_method=vectorizer\n",
    "        )\n",
    "        classifier = classifier_class(random_state=seed)\n",
    "        y_pred_train, y_pred_test, y_train, y_test = train(\n",
    "            classifier,\n",
    "            train_df,\n",
    "            test_df,\n",
    "            Review.target_feature,\n",
    "        )\n",
    "\n",
    "        f1_score_test = f1_score(y_test, y_pred_test, average=\"weighted\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        methods.append(f\"{classifier_class.__name__}-{vectorizer}\")\n",
    "        best_scores.append(f1_score_test)\n",
    "        elapsed_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "bar_width = 0.4\n",
    "index = np.arange(len(methods))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(index, best_scores, width=bar_width)\n",
    "plt.xticks(index, methods, rotation=45)\n",
    "plt.xlabel(\"Methods\")\n",
    "plt.ylabel(\"Best result (F1 score)\")\n",
    "plt.title(\"Quality of the best model\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(index, elapsed_times, width=bar_width)\n",
    "plt.xticks(index, methods, rotation=45)\n",
    "plt.xlabel(\"Methods\")\n",
    "plt.ylabel(\"Execution time (s)\")\n",
    "plt.title(\"Execution time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W eksperymencie porównaliśmy dwa klasyfikatory, maszynę wektorów nośnych (SVM) oraz las losowy, z trzema różnymi wektoryzatorami: Bag-of-Words, TF-IDF i Word2Vec.\n",
    "\n",
    "Wyniki pokazały, że różnice w wynikach F1 między klasyfikatorami nie były znaczące, ale były zauważalne różnice przy porównaniu wektoryzatorów. W szczególności, SVM osiągnął lepsze wyniki z wektoryzatorem TF-IDF niż las losowy z tym samym wektoryzatorem. Ponadto, las losowy osiągnął lepsze wyniki z wektoryzatorem Bag-of-Words niż SVM z tą samą metodą wektoryzacji.\n",
    "\n",
    "Wektoryzator Word2Vec natomiast, potrzebował najwięcej czasu na przetworzenie i nie dał najlepszych wyników dla żadnego z klasyfikatorów. Sugeruje to, że dla tego konkretnego problemu wektoryzatory Bag-of-Words i TF-IDF mogą być bardziej odpowiednie, ponieważ zapewniają lepszą wydajność i wymagają mniej zasobów obliczeniowych.\n",
    "\n",
    "SVM z wektoryzatorem TF-IDF oraz las losowy z wektoryzatorem Bag-of-Words zdają się dawać lepsze rezultaty w tym konkretnym przypadku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_df = preprocess_data(files)\n",
    "# train_df, test_df = split_data(reviews_df, Review.target_feature)\n",
    "\n",
    "# train_df, test_df = extract_features(\n",
    "#     train_df,\n",
    "#     test_df,\n",
    "#     Review.target_feature,\n",
    "#     vectorizer_length = 6\n",
    "# )\n",
    "# classifier = RandomForestClassifier(random_state=seed)\n",
    "# y_pred_train, y_pred_test, y_train, y_test = train(\n",
    "#     classifier,\n",
    "#     train_df,\n",
    "#     test_df,\n",
    "#     Review.target_feature,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# explainer = shap.Explainer(classifier)\n",
    "# shap_values = explainer(test_df.drop(Review.target_feature, axis=1))\n",
    "# shap.summary_plot(shap_values, test_df.drop(Review.target_feature, axis=1))\n",
    "\n",
    "# incorrect_indices = y_test != y_pred_test\n",
    "# incorrect_df = test_df[incorrect_indices]\n",
    "# incorrect_shap_values = shap_values[incorrect_indices]\n",
    "\n",
    "# index_to_analyze = 0\n",
    "# expected_value = explainer.model_output(incorrect_shap_values.base_values[index_to_analyze])\n",
    "\n",
    "# shap.force_plot(\n",
    "#     explainer.expected_value[0],\n",
    "#     incorrect_shap_values[index_to_analyze, :],\n",
    "#     incorrect_df.iloc[index_to_analyze].drop(Review.target_feature),\n",
    "#     feature_names=incorrect_df.drop(Review.target_feature, axis=1).columns,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
